%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}  \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

The Florida iBudget algorithm represents a critical component of the state's developmental disability services infrastructure, determining individual budget allocations for Home and Community-Based Services (HCBS) under the Developmental Disabilities Individual Budgeting waiver program. This system currently serves over 36,000 enrollees, making algorithmic decisions that directly impact the quality of life and service access for individuals with developmental disabilities across Florida. The algorithm's role extends beyond mere budget calculation; it fundamentally shapes how resources are distributed, what services individuals can access, and how person-centered planning principles are implemented in practice.

The enactment of House Bill 1103 in the 2025 legislative session has fundamentally altered the regulatory landscape for iBudget allocation methodologies. This legislation mandates a comprehensive study to review, evaluate, and identify recommendations regarding the current algorithm, with particular emphasis on ensuring compliance with person-centered planning requirements under section 393.0662, Florida Statutes. The bill's requirements extend beyond simple algorithmic refinement, demanding a fundamental reassessment of how statistical methods align with person-centered planning principles and contemporary disability services philosophy.

This analysis addresses three interconnected questions that form the foundation for algorithm evaluation and redesign. 



% First, we examine what the current algorithm accomplishes, including its mathematical formulation, variable selection, and operational mechanics. This examination reveals both the system's statistical foundations and its practical implications for budget determination across diverse disability populations. Second, we identify critical weaknesses in the current approach, ranging from temporal validity issues stemming from outdated data to fundamental limitations in capturing person-centered planning elements. These weaknesses extend beyond technical statistical concerns to encompass broader questions about algorithmic fairness, transparency, and compliance with evolving disability rights frameworks.

% Third, we analyze specific areas where the current algorithm fails to meet the requirements established in House Bill 1103, particularly regarding person-centered planning integration, data currency, and algorithmic robustness. This compliance analysis reveals systematic gaps between the algorithm's actuarial focus and the legislation's emphasis on individualized, preference-driven service planning. The analysis demonstrates that addressing these compliance issues requires more than technical adjustments; it demands a fundamental reconceptualization of how algorithmic systems can support rather than constrain person-centered planning processes.

% The analysis presented in this document extends beyond identifying weaknesses to propose systematic approaches for algorithmic improvement that address both technical limitations and compliance requirements. These approaches range from enhanced linear regression methods that maintain interpretability while improving robustness, to sophisticated machine learning techniques that can capture complex relationships between individual characteristics and support needs, to hybrid approaches that combine statistical prediction with clinical judgment and person-centered planning elements.

% The implementation strategy outlined in this analysis emphasizes phased deployment with comprehensive validation and monitoring to ensure that algorithmic improvements translate into meaningful improvements in service delivery and individual outcomes. This approach recognizes that algorithmic change in disability services carries profound implications for individual wellbeing and requires careful attention to unintended consequences and implementation challenges.

% This comprehensive analysis serves multiple audiences and purposes within Florida's disability services ecosystem. For policymakers and legislative oversight bodies, it provides the technical foundation required by House Bill 1103 while translating complex statistical concepts into policy-relevant insights about algorithmic performance and compliance. For APD administrators and program managers, it offers practical guidance for algorithm selection and implementation while highlighting operational considerations that affect day-to-day service delivery.

% For disability advocacy organizations and individuals receiving services, this analysis provides transparency about algorithmic decision-making processes and identifies specific areas where current methods may not adequately serve person-centered planning principles. For researchers and technical practitioners, it offers detailed methodological analysis and implementation guidance that can inform algorithm development and validation processes.

% The analysis ultimately argues that effective algorithmic systems in disability services require more than statistical sophistication; they demand explicit integration of person-centered planning principles, transparent decision-making processes, and ongoing adaptation to changing service delivery contexts. The current algorithm's limitations stem not merely from technical deficiencies but from a fundamental misalignment between actuarial prediction methods and the individualized, preference-driven approaches that define quality disability services.

% Moving forward, Florida's iBudget system requires algorithmic approaches that can simultaneously achieve statistical rigor, regulatory compliance, person-centered planning integration, and operational practicality. The alternative approaches presented in this analysis offer pathways toward these multiple objectives while acknowledging the inherent tensions and tradeoffs involved in algorithmic design for disability services. The ultimate success of these approaches will depend not only on their technical implementation but on their ability to support rather than constrain the person-centered planning processes that remain central to effective disability services.


% \section{Analysis of the Questionnaire for Situational Information (QSI): Data Types and Model Deficiencies}

% The Florida Questionnaire for Situational Information (QSI) Version 4.0 represents a comprehensive assessment instrument designed to evaluate support needs for individuals with developmental disabilities. This analysis examines the data structure, identifies critical deficiencies in the proposed statistical models, and recommends advanced modeling approaches to address these limitations.

% \subsection{QSI Data Structure and Question Categories}

% The QSI contains comprehensive assessment data organized into three primary domains, each utilizing ordinal scales ranging from 0 (no support needed) to 4 (intensive support required).

% \subsubsection{Functional Status Questions (Q14-Q24)}

% The functional status domain comprises 11 elements assessing daily living support needs:

% \begin{itemize}
%     \item \textbf{Q14 - Vision}: Visual impairment assessment (0=no impairment, 4=constant assistance required)
%     \item \textbf{Q15 - Hearing}: Hearing impairment assessment (0=no impairment, 4=constant assistance required)
%     \item \textbf{Q16 - Eating}: Eating support needs (0=independent, 4=total assistance required)
%     \item \textbf{Q17 - Ambulation}: Mobility support needs (0=independent, 4=constant assistance required)
%     \item \textbf{Q18 - Transfers}: Transfer support needs (0=independent, 4=total assistance required)
%     \item \textbf{Q19 - Toileting}: Toileting support needs (0=independent, 4=total assistance required)
%     \item \textbf{Q20 - Hygiene}: Personal hygiene support needs (0=independent, 4=total assistance required)
%     \item \textbf{Q21 - Dressing}: Dressing support needs (0=independent, 4=total assistance required)
%     \item \textbf{Q22 - Communications}: Communication support needs (0=no impairment, 4=constant assistance required)
%     \item \textbf{Q23 - Self-Protection}: Safety awareness and self-protection (0=independent, 4=constant supervision required)
%     \item \textbf{Q24 - Evacuation Ability}: Emergency evacuation capability (0=independent, 4=total assistance required)
% \end{itemize}

% \subsubsection{Behavioral Status Questions (Q25-Q30)}

% The behavioral domain encompasses 6 elements evaluating intervention needs for challenging behaviors:

% \begin{itemize}
%     \item \textbf{Q25 - Self-Injurious Behavior}: Interventions for self-harm behaviors (0=none required, 4=physical/mechanical restraint used)
%     \item \textbf{Q26 - Aggressive/Hurtful to Others}: Interventions for aggressive behaviors (0=none required, 4=secure facility placement)
%     \item \textbf{Q27 - Destructive to Property}: Interventions for property damage (0=none required, 4=secure facility placement)
%     \item \textbf{Q28 - Inappropriate Sexual Behavior}: Interventions for sexual behavior issues (0=none required, 4=secure facility placement)
%     \item \textbf{Q29 - Running Away}: Interventions for elopement behaviors (0=none required, 4=secure facility placement)
%     \item \textbf{Q30 - Other Behaviors}: Other behaviors leading to separation (0=none required, 4=secure facility placement)
% \end{itemize}

% \subsubsection{Physical Status Questions (Q32-Q50)}

% The physical domain contains 19 elements addressing health and medical concerns:

% \begin{itemize}
%     \item \textbf{Q32 - Self-Injury Related Injuries}: Injury severity from self-injurious behavior
%     \item \textbf{Q33 - Aggression Related Injuries}: Injury severity from aggressive behavior
%     \item \textbf{Q34 - Mechanical Restraints}: Use of protective equipment for behavioral issues
%     \item \textbf{Q35 - Emergency Chemical Restraint}: Use of emergency chemical interventions
%     \item \textbf{Q36 - Psychotropic Medications}: Psychotropic medication usage patterns
%     \item \textbf{Q37 - Gastrointestinal Conditions}: GI-related health issues including reflux, vomiting
%     \item \textbf{Q38 - Seizures}: Seizure-related conditions and management
%     \item \textbf{Q39 - Anti-Epileptic Medications}: Anti-seizure medication usage
%     \item \textbf{Q40 - Skin Breakdown}: Skin integrity issues
%     \item \textbf{Q41 - Bowel Function}: Bowel management needs
%     \item \textbf{Q42 - Nutrition}: Nutritional support requirements
%     \item \textbf{Q43 - Treatment (Physician Prescribed)}: Physician-prescribed treatments
%     \item \textbf{Q44 - Chronic Healthcare Needs}: Assistance with ongoing healthcare management
%     \item \textbf{Q45 - Individual's Injuries}: Personal injury patterns
%     \item \textbf{Q46 - Falls}: Fall-related concerns
%     \item \textbf{Q47 - Physician Visits/Nursing Services}: Healthcare service utilization
%     \item \textbf{Q48 - Emergency Room Visits}: Emergency healthcare utilization
%     \item \textbf{Q49 - Hospital Admissions}: Inpatient healthcare utilization
%     \item \textbf{Q50 - Days Missed}: Activity missed due to illness
% \end{itemize}

% \subsubsection{Composite and Additional Variables}

% The QSI generates several composite scores and includes demographic variables:

% \begin{itemize}
%     \item \textbf{FSum}: Functional status raw score (sum of Q14-Q24, range 0-44)
%     \item \textbf{BSum}: Behavioral status raw score (sum of Q25-Q30, range 0-24)
%     \item \textbf{PSum}: Physical status raw score (sum of Q32-Q50, range 0-76)
%     \item \textbf{Living Setting}: Six categorical levels ranging from family home to intensive residential care
%     \item \textbf{Age Groups}: Multiple categorical classifications (3-20, 21-30, 31+ years)
% \end{itemize}

% \subsection{Structural Inconsistencies in the QSI Assessment Instrument}

% The QSI exhibits several fundamental design inconsistencies that compromise its reliability as a standardized assessment tool. These include non-uniform scaling systems, unvalidated question exclusions, inconsistent temporal frameworks, and ad-hoc scoring rules that violate the instrument's stated ordinal structure.

% \paragraph{Binary vs. Ordinal Scale Inconsistency (Q43)}
% Question 43 (Treatment/physician prescribed) employs a binary scale (0 or 4 only) while all other QSI questions utilize a consistent 5-point ordinal scale (0-4). The standard QSI scaling pattern follows: 0 = none, 1 = minimal, 2 = moderate, 3 = frequent/planned, 4 = intensive. However, Q43 deviates from this structure with only two possible values: 0 = no physician-prescribed procedures required, 4 = requires physician-prescribed procedures carried out by a licensed nurse. This anomaly eliminates intermediate levels 1, 2, and 3, breaking the uniform scaling structure and potentially creating statistical modeling complications due to the bimodal distribution.

% \paragraph{Inconsistent Temporal Assessment Frameworks}
% The questionnaire employs multiple, incompatible time frames across different assessment domains without clear justification for the temporal variations. Behavioral interventions are assessed over the ``past 12 months,'' emergency room visits use a ``last year'' timeframe, hospital admissions reference the ``last six months,'' medication changes examine the ``past year,'' while functional abilities assess ``current status.'' Some items fail to specify any temporal framework entirely. This temporal inconsistency complicates data interpretation and may introduce systematic bias when comparing support needs across different assessment domains.

% \paragraph{Special Scoring Rules Violating Ordinal Structure}
% Several questions employ automatic scoring rules that bypass the standard 0-4 ordinal scale, creating methodological inconsistencies. Q43 mandates an ``automatic score of '4' if physician-prescribed procedures are required,'' while Q36 includes a special provision that ``anyone on Reglan/Metoclopramide, regardless of the reason, has this rating'' of 4. These categorical override rules violate the ordinal measurement principles underlying the assessment instrument and may introduce artificial ceiling effects that distort the distribution of scores and compromise statistical modeling assumptions.

% \paragraph{Version Control and Documentation Issues}
% The questionnaire exhibits evidence of poor version control with conflicting information about revision dates, effective dates, and rule references. The document simultaneously references Version 4.0 as effective 2-15-08 and revised 5-21-15, while mentioning earlier versions with different scaling systems where ``Level 5 that is now identical to Level 4.'' Rule numbers and revision protocols appear inconsistent across different sections of the documentation. This suggests inadequate document management and quality assurance procedures that could lead to implementation inconsistencies across different assessment sites or time periods.

