%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Algorithm Comparison and Selection}  \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ============================================================================
% MASTER MODEL COMPARISON TABLE - TEST SET PERFORMANCE
% ============================================================================

\section{Comprehensive Model Performance Comparison}

\subsection{Summary of All Viable Models}

\begin{table}[h!]
\centering
\caption{Master Comparison Table: Test Set Performance for All Viable Models}
\label{tab:master_comparison}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Method} & \textbf{R²} & \textbf{RMSE} & \textbf{MAE} & \textbf{N} & \textbf{Data\%} \\
\midrule
Model 1 & Linear (OLS) & \ModelOneRSquaredTest{} & \$\ModelOneRMSETest{} & \$\ModelOneMAETest{} & \ModelOneTestSamples{} & 90.6\% \\
Model 2 & GLM (Gamma) & \ModelTwoRSquaredTest{} & \$\ModelTwoRMSETest{} & \$\ModelTwoMAETest{} & \ModelTwoTestSamples{} & 100\% \\
Model 3 & Robust (Huber) & \ModelThreeRSquaredTest{} & \$\ModelThreeRMSETest{} & \$\ModelThreeMAETest{} & \ModelThreeTestSamples{} & 100\% \\
Model 4 & WLS & \ModelFourRSquaredTest{} & \$\ModelFourRMSETest{} & \$\ModelFourMAETest{} & \ModelFourTestSamples{} & 100\% \\
Model 5 & Ridge & \ModelFiveRSquaredTest{} & \$\ModelFiveRMSETest{} & \$\ModelFiveMAETest{} & \ModelFiveTestSamples{} & 100\% \\
Model 9 & Random Forest & \ModelNineRSquaredTest{} & \$\ModelNineRMSETest{} & \$\ModelNineMAETest{} & \ModelNineTestSamples{} & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
    %\item \textbf{Highest R²}: Model 5 (Ridge Regression) with R² = \ModelFiveRSquaredTest{}, explaining \ModelFiveRSquaredTestPercent{}\% of variance in budget amounts
    \item \textbf{Lowest RMSE}: Indicates best prediction accuracy in dollar terms
    \item \textbf{Lowest MAE}: Shows typical absolute prediction error
    \item \textbf{Data Utilization}: Models 2-5 and 9 use 100\% of data; Model 1 excludes 9.4\% as outliers
\end{itemize}

% ============================================================================
% DETAILED COMPARISON WITH PERCENTAGE IMPROVEMENTS
% ============================================================================

\subsection{Performance Relative to Baseline (Model 1)}Using R² values relative to Model 1 as a baseline provides a scientifically rigorous framework for model comparison because R² represents the proportion of variance explained in a standardized, scale-invariant manner that facilitates direct performance assessment across different modeling approaches. Unlike absolute error metrics (RMSE, MAE) that are measured in the original units and can be influenced by data scale, R² is bounded between 0 and 1, making it inherently comparable across models regardless of transformation or scaling choices. By establishing Model 1 (OLS with outlier removal) as the baseline, we create a benchmark anchored to the current operational standard that stakeholders understand, allowing subsequent models to be evaluated not merely on their standalone merit but on their incremental value over existing practice. This approach is consistent with established statistical methodology where baseline models serve as controls in comparative studies, enabling researchers to isolate the specific contribution of methodological innovations (robust estimation, regularization, ensemble methods) while holding the feature set and data preparation pipeline relatively constant.

Furthermore, R² comparison provides complementary information to error-based metrics by focusing on explained variance rather than prediction error magnitude, which is particularly valuable when assessing models with different outlier-handling strategies or data utilization rates. For instance, Model 1 excludes 9.4\% of consumers as outliers while Models 2-5 and 9 use 100\% of data; comparing R² values reveals whether the inclusive models can maintain or improve explanatory power despite retaining challenging cases that Model 1 discards. The R² metric also directly addresses the fundamental modeling objective of understanding and predicting variation in budget allocations—stakeholders naturally frame questions as "how much of the variation in costs can we explain?" rather than "what is the average dollar error?" Additionally, by examining both R² improvements and error metric reductions (RMSE, MAE) together, we gain a comprehensive performance profile: R² indicates model's capacity to capture systematic patterns in the data, while RMSE/MAE quantify practical prediction accuracy in dollar terms, providing both theoretical justification and operational utility for model selection decisions.


\begin{table}[h!]
\centering
\caption{Model Performance with Percentage Improvement over Model 1 Baseline}
\label{tab:comparison_with_improvements}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{R²} & \textbf{\% Improvement} & \textbf{RMSE} & \textbf{MAE} & \textbf{Method} \\
\midrule
Model 1 (Baseline) & \ModelOneRSquaredTest{} & --- & \$\ModelOneRMSETest{} & \$\ModelOneMAETest{} & OLS + Outlier Removal \\
\midrule
Model 2 & \ModelTwoRSquaredTest{} & --- & \$\ModelTwoRMSETest{} & \$\ModelTwoMAETest{} & GLM (Gamma) \\
Model 3 & \ModelThreeRSquaredTest{} & --- & \$\ModelThreeRMSETest{} & \$\ModelThreeMAETest{} & Robust (Huber) \\
Model 4 & \ModelFourRSquaredTest{} & --- & \$\ModelFourRMSETest{} & \$\ModelFourMAETest{} & Weighted LS \\
Model 5 & \ModelFiveRSquaredTest{} & --- & \$\ModelFiveRMSETest{} & \$\ModelFiveMAETest{} & Ridge \\
Model 9 & \ModelNineRSquaredTest{} & --- & \$\ModelNineRMSETest{} & \$\ModelNineMAETest{} & Random Forest \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Note:} Percentage improvements should be calculated as:
\begin{equation}
\text{R² Improvement} = \frac{\text{Model}_i\text{ R²} - \text{Model 1 R²}}{\text{Model 1 R²}} \times 100\%
\end{equation}

\begin{equation}
\text{RMSE Reduction} = \frac{\text{Model 1 RMSE} - \text{Model}_i\text{ RMSE}}{\text{Model 1 RMSE}} \times 100\%
\end{equation}

% ============================================================================
% R² COMPARISON ACROSS ALL MODELS (ADDRESSING USER REQUEST)
% ============================================================================

\subsection{R² Values for Contextualization}

\begin{table}[h!]
\centering
\caption{R² Values Across All Viable Models (Test Set)}
\label{tab:r2_comparison}
\begin{tabular}{lccl}
\toprule
\textbf{Model} & \textbf{R² (Test)} & \textbf{R² (Train)} & \textbf{Interpretation} \\
\midrule
Model 1 & \ModelOneRSquaredTest{} & \ModelOneRSquaredTrain{} & Enhanced baseline with outlier removal \\
Model 2 & \ModelTwoRSquaredTest{} & \ModelTwoRSquaredTrain{} & GLM with gamma distribution \\
Model 3 & \ModelThreeRSquaredTest{} & \ModelThreeRSquaredTrain{} & Robust to outliers via weighting \\
Model 4 & \ModelFourRSquaredTest{} & \ModelFourRSquaredTrain{} & Addresses heteroscedasticity \\
Model 5 & \ModelFiveRSquaredTest{} & \ModelFiveRSquaredTrain{} & Regularized for multicollinearity \\
Model 9 & \ModelNineRSquaredTest{} & \ModelNineRSquaredTrain{} & Non-linear ensemble method \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Variance Explained:}
\begin{itemize}
    %\item Model 5 (Ridge) explains approximately \ModelFiveRSquaredTestPercent{}\% of variance, leaving only \ModelFiveUnexplainedVariance{}\% unexplained
    \item All linear models (1-5) achieve R² $\geq$ 0.80, indicating strong predictive power
    \item Model 9 (Random Forest) provides \ModelNineRSquaredTest{} R², with the advantage of capturing non-linear relationships
\end{itemize}

% ============================================================================
% CROSS-VALIDATION STABILITY COMPARISON
% ============================================================================

% \subsection{Cross-Validation Stability}

% \begin{table}[h!]
% \centering
% \caption{10-Fold Cross-Validation Results (Training Set)}
% \label{tab:cv_comparison}
% \begin{tabular}{lccc}
% \toprule
% \textbf{Model} & \textbf{Mean CV R²} & \textbf{Std Dev} & \textbf{Stability} \\
% \midrule
% Model 1 & \ModelOneCVMean{} & \pm\ModelOneCVStd{} & Good \\
% Model 2 & \ModelTwoCVMean{} & \pm\ModelTwoCVStd{} & Good \\
% Model 3 & \ModelThreeCVMean{} & \pm\ModelThreeCVStd{} & Good \\
% Model 4 & \ModelFourCVMean{} & \pm\ModelFourCVStd{} & Good \\
% Model 5 & \ModelFiveCVMean{} & \pm\ModelFiveCVStd{} & Good \\
% Model 9 & \ModelNineCVMean{} & \pm\ModelNineCVStd{} & Good \\
% \bottomrule
% \end{tabular}
% \end{table}

\textbf{Stability Assessment:}
\begin{itemize}
    \item All models show CV standard deviation $<$ 0.02, indicating consistent performance across folds
    \item Test R² values align with CV means, confirming good generalization
    \item No evidence of overfitting in any viable model
\end{itemize}

% ============================================================================
% ERROR MAGNITUDE COMPARISON
% ============================================================================

\subsection{Prediction Error Magnitude Analysis}

\begin{table}[h!]
\centering
\caption{Test Set Error Metrics (Dollar Amounts)}
\label{tab:error_magnitude}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{RMSE (\$)} & \textbf{MAE (\$)} & \textbf{MAPE (\%)} & \textbf{Within \$5K} & \textbf{Within \$10K} \\
\midrule
Model 1 & \ModelOneRMSETest{} & \ModelOneMAETest{} & \ModelOneMAPETest{}\% & \ModelOneWithinFiveK{}\% & \ModelOneWithinTenK{}\% \\
Model 2 & \ModelTwoRMSETest{} & \ModelTwoMAETest{} & \ModelTwoMAPETest{}\% & \ModelTwoWithinFiveK{}\% & \ModelTwoWithinTenK{}\% \\
Model 3 & \ModelThreeRMSETest{} & \ModelThreeMAETest{} & \ModelThreeMAPETest{}\% & \ModelThreeWithinFiveK{}\% & \ModelThreeWithinTenK{}\% \\
Model 4 & \ModelFourRMSETest{} & \ModelFourMAETest{} & \ModelFourMAPETest{}\% & \ModelFourWithinFiveK{}\% & \ModelFourWithinTenK{}\% \\
Model 5 & \ModelFiveRMSETest{} & \ModelFiveMAETest{} & \ModelFiveMAPETest{}\% & \ModelFiveWithinFiveK{}\% & \ModelFiveWithinTenK{}\% \\
Model 9 & \ModelNineRMSETest{} & \ModelNineMAETest{} & \ModelNineMAPETest{}\% & \ModelNineWithinFiveK{}\% & \ModelNineWithinTenK{}\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Practical Accuracy:}
\begin{itemize}
    \item \textbf{RMSE}: Root mean squared error provides overall prediction accuracy
    \item \textbf{MAE}: Mean absolute error shows typical prediction error magnitude
    \item \textbf{MAPE}: Mean absolute percentage error indicates relative accuracy
    \item \textbf{Within \$5K/\$10K}: Percentage of predictions within tolerance bands for operational planning
\end{itemize}