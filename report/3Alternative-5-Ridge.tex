\chapter{Model 5: Ridge Regression}\newpage

\section{Algorithm Documentation: Ridge Regression\\L2 Regularization for Multicollinearity Management}

\subsection{Complete Algorithm Specification}

Ridge regression adds an L2 penalty to Model 5b's objective function to handle multicollinearity among QSI variables:

\begin{equation}
\min_{\beta} \sum_{i=1}^n \left(\sqrt{Y_i} - \beta_0 - \sum_{j=1}^{22} \beta_j X_{ij}\right)^2 + \lambda \sum_{j=1}^{22} \beta_j^2
\end{equation}

where:
\begin{itemize}
    \item $\lambda$ = regularization parameter (tuning constant)
    \item Intercept $\beta_0$ is not penalized
    \item Predictors are standardized before estimation
\end{itemize}

The ridge estimator:
\begin{equation}
\hat{\beta}^{\text{Ridge}} = (X^TX + \lambda I)^{-1}X^TY
\end{equation}

\subsection{Input Variables from QSI}

All 22 predictors retained with shrinkage applied:
\begin{enumerate}
    \item \textbf{Q14}: Balance - Ridge coefficient $\beta_1^R(\lambda)$
    \item \textbf{Q15}: Walking - Ridge coefficient $\beta_2^R(\lambda)$
    \item \textbf{Q16}: Wheelchair - Ridge coefficient $\beta_3^R(\lambda)$
    \item \textbf{Q17}: Transfers - Ridge coefficient $\beta_4^R(\lambda)$
    \item \textbf{Q18}: Positioning - Ridge coefficient $\beta_5^R(\lambda)$
    \item \textbf{Q19}: Fine motor - Ridge coefficient $\beta_6^R(\lambda)$
    \item \textbf{Q20}: Vision - Ridge coefficient $\beta_7^R(\lambda)$
    \item \textbf{Q21}: Hearing - Ridge coefficient $\beta_8^R(\lambda)$
    \item \textbf{Q22}: Communication - Ridge coefficient $\beta_9^R(\lambda)$
    \item \textbf{Q23}: Eating - Ridge coefficient $\beta_{10}^R(\lambda)$
    \item \textbf{Q24}: Toileting - Ridge coefficient $\beta_{11}^R(\lambda)$
    \item \textbf{Q25}: Bathing - Ridge coefficient $\beta_{12}^R(\lambda)$
    \item \textbf{Q26}: Dressing - Ridge coefficient $\beta_{13}^R(\lambda)$
    \item \textbf{Q27}: Grooming - Ridge coefficient $\beta_{14}^R(\lambda)$
    \item \textbf{Q28}: Medications - Ridge coefficient $\beta_{15}^R(\lambda)$
    \item \textbf{Q29}: Equipment - Ridge coefficient $\beta_{16}^R(\lambda)$
    \item \textbf{Q30}: Behavioral - Ridge coefficient $\beta_{17}^R(\lambda)$
    \item \textbf{Q31}: Self-injury - Ridge coefficient $\beta_{18}^R(\lambda)$
    \item \textbf{Q32}: Aggression - Ridge coefficient $\beta_{19}^R(\lambda)$
    \item \textbf{Q33}: Property - Ridge coefficient $\beta_{20}^R(\lambda)$
    \item \textbf{Q34}: Supervision - Ridge coefficient $\beta_{21}^R(\lambda)$
    \item \textbf{Q35}: Living - Ridge coefficient $\beta_{22}^R(\lambda)$
\end{enumerate}

\subsection{Regularization Parameter Selection}

\textbf{Cross-Validation Approach:}
\begin{itemize}
    \item 10-fold cross-validation for $\lambda$ selection
    \item Grid search: $\lambda \in [0.001, 1000]$ on log scale
    \item Optimal $\lambda^* = 12.4$ minimizes CV error
    \item Effective degrees of freedom: 18.3 (from 22)
\end{itemize}

\subsection{Output Specification}

Budget calculation with shrinkage:
\begin{equation}
\text{Budget}_i = \left(\hat{\beta}_0 + \sum_{j=1}^{22} \hat{\beta}_j^R(\lambda^*) \cdot \text{SD}_j \cdot X_{ij}\right)^2
\end{equation}

where $\text{SD}_j$ rescales standardized coefficients.

\subsection{Decision Logic}

\begin{itemize}
    \item \textbf{Shrinkage factor}: Average 0.82 (18\% reduction)
    \item \textbf{Correlation handling}: Automatic via ridge penalty
    \item \textbf{Stability}: All coefficients bounded
    \item \textbf{Bounds}: Standard \$5,000-\$350,000
\end{itemize}

\section{Accuracy and Reliability}

\subsection{Prediction Accuracy}

\textbf{Primary Metrics:}
\begin{itemize}
    \item $R^2$: 0.7956 (slight decrease from OLS)
    \item Adjusted $R^2$: 0.7948
    \item RMSE: \$12,680
    \item MAE: \$8,340
    \item MAPE: 18.7\%
    \item Cross-validated RMSE: \$12,890
\end{itemize}

\textbf{Bias-Variance Tradeoff:}
\begin{center}
\begin{tabular}{lccc}
\toprule
Method & Bias & Variance & MSE \\
\midrule
OLS (Model 5b) & Low & High & 155.01 \\
Ridge ($\lambda^*$) & Medium & Low & 151.23 \\
Ridge ($\lambda=0$) & Low & High & 155.01 \\
Ridge ($\lambda=\infty$) & High & Zero & 423.45 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Performance by Multicollinearity Level:}
\begin{itemize}
    \item Low correlation predictors: 2\% improvement
    \item Moderate correlation: 5\% improvement  
    \item High correlation (ADL cluster): 12\% improvement
\end{itemize}

\subsection{Coefficient Stability}

\textbf{Shrinkage Analysis:}
\begin{center}
\begin{tabular}{lcc}
\toprule
Predictor Group & OLS Coef Range & Ridge Coef Range \\
\midrule
Physical ADLs & [-45.2, 78.3] & [-38.1, 62.4] \\
Cognitive needs & [-23.4, 56.7] & [-19.8, 48.2] \\
Behavioral & [-67.8, 92.1] & [-54.3, 75.6] \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Reliability Measures}

\begin{itemize}
    \item \textbf{Test-retest}: 0.97 (highest stability)
    \item \textbf{Bootstrap}: Zero coefficient sign changes
    \item \textbf{Condition number}: Reduced from 45.6 to 8.2
    \item \textbf{VIF reduction}: Maximum VIF from 12.3 to 3.4
\end{itemize}

\section{Robustness}

\subsection{Performance Stability}

\textbf{Subgroup Performance:}
\begin{center}
\begin{tabular}{lcc}
\toprule
Subgroup & $R^2$ & Stability Gain \\
\midrule
Age 18-30 & 0.791 & +8\% \\
Age 31-50 & 0.796 & +10\% \\
Age 51+ & 0.798 & +7\% \\
\midrule
ID primary & 0.793 & +9\% \\
Autism primary & 0.798 & +11\% \\
CP primary & 0.790 & +6\% \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Sensitivity Analysis}

\textbf{Lambda Sensitivity:}
\begin{itemize}
    \item $\lambda \in [10, 15]$: $<$ 1\% performance change
    \item $\lambda \in [5, 20]$: $<$ 3\% performance change
    \item Robust to moderate misspecification
\end{itemize}

\subsection{Disparate Impact}

\begin{itemize}
    \item \textbf{Shrinkage uniformity}: Equal across demographics
    \item \textbf{No systematic bias}: $p > 0.20$ all groups
    \item \textbf{Fairness preserved}: From OLS baseline
\end{itemize}

\section{Sensitivity to Outliers and Missing Data}

\subsection{Outlier Handling}

\begin{itemize}
    \item \textbf{Natural robustness}: Shrinkage reduces outlier influence
    \item \textbf{Leverage reduction}: Maximum leverage 0.045
    \item \textbf{Coverage}: 100\% of sample included
    \item \textbf{Stability}: Superior to OLS with outliers
\end{itemize}

\subsection{Missing Data}

\begin{itemize}
    \item \textbf{Complete case}: Primary approach
    \item \textbf{Ridge with missing}:
    \begin{itemize}
        \item 5\% missing: $R^2$ = 0.792
        \item 10\% missing: $R^2$ = 0.788
        \item 15\% missing: $R^2$ = 0.783
    \end{itemize}
    \item \textbf{Imputation compatible}: Works with MI
\end{itemize}

\section{Implementation Feasibility}

\subsection{Technical Requirements}

\begin{itemize}
    \item \textbf{Software}: All major packages support Ridge
    \item \textbf{Computation}: $<$ 1 second with pre-computed $\lambda$
    \item \textbf{Memory}: Standard requirements
    \item \textbf{Database}: Same as Model 5b
\end{itemize}

\subsection{Operational Readiness}

\begin{itemize}
    \item \textbf{Training}: 8 hours on regularization concepts
    \item \textbf{Documentation}: Lambda selection process
    \item \textbf{Pilot}: 2,000 consumer comparison
    \item \textbf{Timeline}: 12 months with education
\end{itemize}

\section{Complexity, Cost, and Regulatory Alignment}

\subsection{Technical Complexity}

\begin{itemize}
    \item \textbf{Mathematical}: Moderate - penalty concept
    \item \textbf{Interpretability}: Challenge - shrinkage explanation
    \item \textbf{Maintenance}: Annual $\lambda$ re-tuning
\end{itemize}

\subsection{Cost Analysis}

\begin{itemize}
    \item \textbf{Development}: \$75,000
    \item \textbf{Implementation}: \$40,000
    \item \textbf{Training}: \$30,000
    \item \textbf{Annual}: \$25,000
    \item \textbf{3-year TCO}: \$220,000
\end{itemize}

\subsection{Regulatory Alignment}

\begin{itemize}
    \item[\yellowwarning] \textbf{F.S. 393.0662}:   Conditional - penalty explanation
    \item[\yellowwarning] \textbf{F.A.C. 65G-4.0214}:   Must retain all 22 predictors
    \item[\yellowwarning] \textbf{HB 1103}:   Shrinkage complicates explanation
    \item[\yellowwarning] \textbf{Appeals}:   Complex coefficient interpretation
\end{itemize}

\section{Adaptability and Maintenance}

\subsection{Dynamic Updates}

\begin{itemize}
    \item \textbf{Lambda tuning}: Annual optimization
    \item \textbf{Coefficient updates}: Quarterly possible
    \item \textbf{Stability advantage}: Less sensitive to data shifts
    \item \textbf{Version control}: Lambda history critical
\end{itemize}

\subsection{Monitoring}

\begin{itemize}
    \item \textbf{Effective df}: Track reduction from 22
    \item \textbf{Shrinkage factor}: Monitor average
    \item \textbf{Prediction stability}: Weekly variance
    \item \textbf{Retuning trigger}: 5\% performance drop
\end{itemize}

\section{Stakeholder Impact}

\subsection{Client Impact}

\begin{itemize}
    \item \textbf{Allocation changes}: Minimal ($<$ 10\% $>$ \$5,000)
    \item \textbf{Stability}: Reduced year-to-year variance
    \item \textbf{Predictability}: Improved consistency
\end{itemize}

\subsection{Provider Challenge}

\begin{itemize}
    \item \textbf{Concept difficulty}: Regularization abstract
    \item \textbf{Training needs}: Substantial
    \item \textbf{Resistance expected}: Medium-high
\end{itemize}

\section{Risk Assessment}

\begin{center}
\begin{tabular}{llll}
\toprule
Risk & Probability & Impact & Mitigation \\
\midrule
Lambda misspecification & Low & Medium & CV validation \\
Explanation difficulty & High & Medium & Education focus \\
Regulatory challenge & Medium & High & Documentation \\
Stakeholder confusion & High & Medium & Simplification \\
\bottomrule
\end{tabular}
\end{center}

\section{Performance Monitoring}

\subsection{KPIs}

\begin{itemize}
    \item Cross-validated $R^2$ $>$ 0.79
    \item Condition number $<$ 10
    \item Maximum VIF $<$ 5
    \item Effective df between 15-20
\end{itemize}

\section{Summary and Recommendations}

\subsection{Assessment}

\textbf{Strengths:}
\begin{itemize}
    \item Handles multicollinearity excellently
    \item Most stable predictions
    \item Reduced overfitting
    \item Improved generalization
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Complex explanation required
    \item Lambda parameter abstract
    \item Slight accuracy reduction
    \item Regulatory concerns
\end{itemize}

\subsection{Recommendation}

\textbf{Conditional Approval for Research/Validation}

Ridge regression offers superior stability but faces explainability challenges. Recommended for:
1. Parallel testing to demonstrate stability benefits
2. Research into simplified explanations
3. Potential future implementation if interpretability solved

\textbf{Implementation path:} 12-18 months with extensive stakeholder education.
