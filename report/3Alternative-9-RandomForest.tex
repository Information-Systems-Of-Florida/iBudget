% 3Alternative-9-RandomForest.tex
% Model 9: Random Forest Regression
% Florida APD iBudget Algorithm Calibration Project

\chapter{Model 9: Random Forest Regression}\label{ch:model9}

% Load model-specific values
\input{models/model_9/model_9_renewcommands.tex}

% Setup template - CRITICAL: Use correct model word
\SetupModelTemplate{Nine}

% Store model number
\def\themodel{9}

\section{Executive Summary}

Model 9 employs Random Forest regression, an ensemble learning method that combines predictions from \ModelNineNTrees{} decision trees trained on bootstrap samples of the data. This non-parametric approach automatically captures complex non-linear relationships and feature interactions without requiring explicit mathematical specification, while maintaining natural robustness to outliers and extreme values.

\subsection{Purpose and Scope}

The Random Forest model addresses key limitations of linear approaches by:

\begin{itemize}
    \item \textbf{Automatic Interaction Detection}: Discovers feature interactions without manual specification through recursive partitioning
    \item \textbf{Non-Linear Modeling}: Captures complex relationships that linear models cannot represent
    \item \textbf{Natural Robustness}: Ensemble averaging provides inherent protection against outliers
    \item \textbf{100\% Data Utilization}: No consumer exclusions required, improving fairness
    \item \textbf{Built-In Validation}: Out-of-bag (OOB) error estimation provides ongoing performance monitoring
    \item \textbf{Interpretability Through Importance}: Feature importance rankings facilitate policy understanding
\end{itemize}

\subsection{Key Findings}

\begin{itemize}
    \item \textbf{Model 9 Performance}: Test R$^2$ = \ModelNineRSquaredTest{}, RMSE = \$\ModelNineRMSETest{}, demonstrating strong predictive accuracy
    \item \textbf{Transformation}: Uses \ModelNineTransformation{} transformation
    \item \textbf{Accuracy Bands}: \ModelNineWithinFiveK{}\% of predictions within \$5,000, \ModelNineWithinTenK{}\% within \$10,000
    \item \textbf{Data Retention}: 100\% (no outlier removal -- Random Forest naturally robust to extreme values)
    \item \textbf{Cross-Validation}: 10-fold CV R$^2$ = \ModelNineCVMean{} $\pm$ \ModelNineCVStd{}
    \item \textbf{Out-of-Bag Validation}: OOB R$^2$ = \ModelNineOOBRSquared{}, RMSE = \$\ModelNineOOBError{}
    \item \textbf{Feature Set}: \ModelNineNumFeatures{} robust predictors (living settings, age groups, key QSI items, behavioral summary scores)
    \item \textbf{Computational Efficiency}: Training completed in \ModelNineTrainingTime{} seconds on standard hardware
    \item \textbf{Top Predictor}: \ModelNineTopFeatureOne{} (importance: \ModelNineTopFeatureOneImportance{})
\end{itemize}

The Random Forest model demonstrates robust performance across demographic subgroups while automatically discovering complex feature interactions. Natural outlier handling ensures fair treatment of all consumers without data exclusion, a critical advantage for policy equity.

\section{Methodological Foundation}

\subsection{Random Forest Framework}

Random Forest creates an ensemble of decision trees, each trained independently on a bootstrap sample (random sample with replacement) of the training data. For regression problems, the final prediction is the average of all individual tree predictions:

\begin{equation}
\hat{y}_{\text{RF}}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{y}_b(x)
\end{equation}

where $B$ is the number of trees (\ModelNineNTrees{} in this implementation) and $\hat{y}_b(x)$ is the prediction from the $b$-th tree.

\subsection{Bootstrap Aggregation (Bagging)}

Each tree is trained on a bootstrap sample containing approximately 63.2\% of unique observations (due to sampling with replacement). The remaining 36.8\% of observations, called out-of-bag (OOB) samples, serve as a natural validation set for that tree. The OOB error estimate is computed by:

\begin{enumerate}
    \item For each observation $i$, collect predictions only from trees where $i$ was OOB
    \item Average these predictions to get OOB prediction for observation $i$
    \item Calculate error metrics using actual vs. OOB predicted values
\end{enumerate}

The OOB estimate provides an unbiased assessment of model performance without requiring a separate validation set.

\subsection{Recursive Binary Partitioning}

Each decision tree is built by recursively partitioning the feature space:

\begin{enumerate}
    \item At each node, consider a random subset of \ModelNineMaxFeatures{} features
    \item For each candidate feature $j$ and split point $s$, evaluate:
    \begin{equation}
    \text{RSS}_{\text{split}} = \sum_{x_i \in R_L(j,s)} (y_i - \bar{y}_L)^2 + \sum_{x_i \in R_R(j,s)} (y_i - \bar{y}_R)^2
    \end{equation}
    where $R_L$ and $R_R$ are left and right regions, $\bar{y}_L$ and $\bar{y}_R$ are region means
    \item Select the split $(j^*, s^*)$ that minimizes RSS
    \item Apply stopping criteria: minimum samples per leaf = \ModelNineMinSamplesLeaf{}, minimum samples to split = \ModelNineMinSamplesSplit{}
    \item Continue until stopping criteria met or maximum depth (\ModelNineMaxDepth{}) reached
\end{enumerate}

\subsection{Feature Importance}

Random Forests provide feature importance through mean decrease in impurity:

\begin{equation}
\text{Importance}(X_j) = \frac{1}{B} \sum_{b=1}^{B} \sum_{t \in T_b} \mathbb{I}(\text{split on } X_j \text{ at node } t) \times \Delta \text{RSS}_t
\end{equation}

where $T_b$ is the set of internal nodes in tree $b$, and $\Delta \text{RSS}_t$ is the reduction in residual sum of squares at node $t$.

\subsection{Advantages for iBudget Modeling}

\subsubsection{Automatic Interaction Detection}

Unlike linear models that require manual specification of interaction terms (e.g., Living Setting $\times$ FSum), Random Forests automatically discover interactions through the tree structure. If FSum's effect differs by living setting, trees will naturally split on living setting before FSum (or vice versa).

\subsubsection{Robustness to Outliers}

Two mechanisms provide natural robustness:

\begin{enumerate}
    \item \textbf{Median Splits}: Tree splits use rank-based decisions, making them insensitive to extreme values
    \item \textbf{Ensemble Averaging}: Even if individual trees overfit to outliers, averaging across $B$ trees reduces their influence
\end{enumerate}

This eliminates the need for outlier removal procedures, ensuring 100\% data utilization and improved fairness.

\subsubsection{Non-Linear Relationships}

Trees can capture non-linear patterns that linear models miss. For example, if BSum's effect plateaus at high values (diminishing returns), trees naturally model this through appropriate splits.

\subsection{Mathematical Framework}

For consumer $i$ with features $\mathbf{x}_i = (x_{i1}, \ldots, x_{ip})$, the Random Forest prediction is:

\begin{equation}
\hat{y}_i = \frac{1}{B} \sum_{b=1}^{B} f_b(\mathbf{x}_i)
\end{equation}

where $f_b(\cdot)$ is the $b$-th tree function. Each tree $f_b$ partitions the feature space into $M_b$ regions $\{R_{1b}, R_{2b}, \ldots, R_{M_b b}\}$ and assigns a constant prediction to each region:

\begin{equation}
f_b(\mathbf{x}) = \sum_{m=1}^{M_b} c_{mb} \cdot \mathbb{I}(\mathbf{x} \in R_{mb})
\end{equation}

where $c_{mb}$ is the average response of training observations in region $R_{mb}$.

\subsection{Model Configuration}

This implementation uses the following configuration:

\begin{itemize}
    \item \textbf{Number of Trees}: \ModelNineNTrees{} (provides stable predictions through sufficient averaging)
    \item \textbf{Maximum Depth}: \ModelNineMaxDepth{} (allows trees to capture complex patterns)
    \item \textbf{Min Samples Split}: \ModelNineMinSamplesSplit{} (prevents overfitting to noise)
    \item \textbf{Min Samples Leaf}: \ModelNineMinSamplesLeaf{} (ensures reasonable prediction stability)
    \item \textbf{Max Features per Split}: \ModelNineMaxFeatures{} (introduces diversity among trees)
    \item \textbf{Bootstrap}: Yes (enables OOB validation)
    \item \textbf{Mean Tree Depth}: \ModelNineMeanTreeDepth{} (indicates model complexity)
\end{itemize}

% ============================================
% INSERT UNIVERSAL TEMPLATE HERE
% ============================================
\input{model_template.tex}

% ============================================
% MODEL-SPECIFIC CONTENT BELOW
% ============================================

\section{Model 9 Specific Analysis}

\subsection{Feature Importance Analysis}

Random Forest's automatic feature importance ranking provides valuable policy insights without requiring manual interaction specification. The top 5 most important features are:

\begin{enumerate}
    \item \textbf{\ModelNineTopFeatureOne{}}: Importance = \ModelNineTopFeatureOneImportance{}
    \item \textbf{\ModelNineTopFeatureTwo{}}: Importance = \ModelNineTopFeatureTwoImportance{}
    \item \textbf{\ModelNineTopFeatureThree{}}: Importance = \ModelNineTopFeatureThreeImportance{}
    \item \textbf{\ModelNineTopFeatureFour{}}: Importance = \ModelNineTopFeatureFourImportance{}
    \item \textbf{\ModelNineTopFeatureFive{}}: Importance = \ModelNineTopFeatureFiveImportance{}
\end{enumerate}

Feature importance is calculated as the total reduction in residual sum of squares (RSS) attributed to splits on that feature, averaged across all \ModelNineNTrees{} trees. Higher values indicate greater predictive power.

\subsection{Advantages Over Linear Models}

\subsubsection{Discovered vs. Specified Interactions}

Linear models (Models 1-5) include three manually-specified interaction terms:
\begin{itemize}
    \item Living in Family Home $\times$ FSum
    \item Living in Supported Living $\times$ FSum  
    \item Living in Supported Living $\times$ BSum
\end{itemize}

Random Forest automatically discovers these and potentially many other relevant interactions through its tree structure. If QSI item 28 has different effects for different age groups, trees will naturally capture this pattern without explicit specification.

\subsubsection{Non-Linear Pattern Capture}

Consider BSum (behavioral support needs). A linear model assumes each additional point of BSum has the same cost impact. Random Forest can capture:
\begin{itemize}
    \item \textbf{Threshold Effects}: Little cost impact until BSum exceeds a critical value
    \item \textbf{Diminishing Returns}: Large initial effect that plateaus at high values
    \item \textbf{Conditional Effects}: BSum's impact varies by other consumer characteristics
\end{itemize}

This flexibility improves predictions for consumers with unusual combinations of characteristics.

\subsubsection{Natural Outlier Robustness}

Models 1-5 require outlier removal to achieve optimal performance, excluding approximately 10\% of consumers. Random Forest's ensemble structure provides natural robustness:

\begin{enumerate}
    \item Individual trees may be affected by extreme values in their bootstrap sample
    \item Other trees, trained on different samples, are unaffected
    \item Averaging across \ModelNineNTrees{} trees dampens the influence of any single outlier
    \item Median-based splits (rank ordering) are inherently robust to extreme values
\end{enumerate}

Result: 100\% data utilization without sacrificing predictive accuracy.

\subsection{Out-of-Bag Validation}

The OOB error estimate (R$^2$ = \ModelNineOOBRSquared{}, RMSE = \$\ModelNineOOBError{}) provides several advantages:

\begin{itemize}
    \item \textbf{Unbiased Performance Estimate}: Each prediction uses only trees where that observation was not in the training sample
    \item \textbf{No Separate Validation Set Needed}: Maximizes training data utilization
    \item \textbf{Ongoing Monitoring}: Can be computed continuously during production use
    \item \textbf{Agreement with Test Set}: OOB metrics closely match test set performance, validating both estimates
\end{itemize}

\subsection{Computational Considerations}

\subsubsection{Training Efficiency}

Model 9 completed training in \ModelNineTrainingTime{} seconds on standard hardware. While slower than linear models (<1 second), this remains acceptable for:

\begin{itemize}
    \item Annual recalibration cycles (training once per year)
    \item Near-instantaneous prediction (milliseconds per consumer)
    \item Parallel processing capability (used all available CPU cores)
\end{itemize}

\subsubsection{Prediction Speed}

Once trained, Random Forest prediction is extremely fast:
\begin{itemize}
    \item Each tree evaluates simple comparison operations (is feature $X_j > s$?)
    \item All trees can be evaluated in parallel
    \item Average prediction over trees is computationally trivial
    \item Typical prediction time: <1 millisecond per consumer
\end{itemize}

Production deployment can easily handle real-time prediction for all Florida APD consumers.

\subsection{Trade-offs and Limitations}

\subsubsection{Interpretability}

While feature importance provides global interpretability, understanding individual predictions requires additional tools:

\begin{itemize}
    \item \textbf{SHAP Values}: Explain individual predictions by showing each feature's contribution
    \item \textbf{Partial Dependence Plots}: Visualize average effect of each feature
    \item \textbf{Individual Tree Paths}: Track decision path for specific consumers
\end{itemize}

Implementation of these explainability tools is recommended for regulatory compliance and stakeholder communication.

\subsubsection{Extrapolation Limitations}

Random Forests cannot extrapolate beyond the range of training data:

\begin{itemize}
    \item Predictions are bounded by min/max training values in each region
    \item New consumer types (e.g., unprecedented combinations of characteristics) may not be predicted accurately
    \item Regular retraining with current data mitigates this limitation
\end{itemize}

\subsubsection{Computational Requirements}

While prediction is fast, training and storage requirements are higher than linear models:

\begin{itemize}
    \item \textbf{Training Time}: \ModelNineTrainingTime{} seconds vs. <1 second for OLS
    \item \textbf{Model Size}: \ModelNineNTrees{} trees must be stored vs. one set of coefficients
    \item \textbf{Infrastructure}: Requires Python/scikit-learn environment
\end{itemize}

However, these requirements remain modest for modern computing infrastructure.

\subsection{Policy Implications}

\subsubsection{Discovered Interactions Inform Policy}

Feature importance analysis reveals which interactions matter most for cost prediction. This information can guide policy decisions:

\begin{itemize}
    \item High importance of specific QSI items suggests these are key cost drivers
    \item Living setting effects can be quantified without assuming linear relationships
    \item Age effects may be more complex than simple binary categories suggest
\end{itemize}

\subsubsection{Equitable Treatment Through 100\% Data Inclusion}

By avoiding outlier removal, Model 9 ensures:

\begin{itemize}
    \item All consumers receive evidence-based budget predictions
    \item No systematic exclusion of high-cost or unusual cases
    \item Natural handling of legitimate extreme support needs
    \item Improved fairness and defensibility of allocations
\end{itemize}

\subsubsection{Adaptation to Policy Changes}

Random Forest's non-parametric nature allows it to adapt to policy changes without model restructuring:

\begin{itemize}
    \item If a new support type is introduced, simply add as a feature
    \item If QSI scoring changes, model automatically adjusts to new patterns
    \item No need to hypothesize and test specific interaction terms
\end{itemize}

\subsection{Comparison with Alternative Models}

\begin{table}[h]
\centering
\caption{Model 9 vs. Alternative Approaches}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Test R$^2$} & \textbf{Data Use} & \textbf{Interactions} & \textbf{Training Time} \\
\midrule
Model 1 (OLS) & \ModelOneRSquaredTest{} & 90.6\% & 3 (manual) & <1 sec \\
Model 3 (Robust) & \ModelThreeRSquaredTest{} & 100\% & 3 (manual) & <1 sec \\
Model 4 (WLS) & \ModelFourRSquaredTest{} & 100\% & 3 (manual) & <2 sec \\
Model 5 (Ridge) & \ModelFiveRSquaredTest{} & 90.6\% & 3 (manual) & <1 sec \\
\textbf{Model 9 (RF)} & \textbf{\ModelNineRSquaredTest{}} & \textbf{100\%} & \textbf{Automatic} & \textbf{\ModelNineTrainingTime{} sec} \\
\bottomrule
\end{tabular}
\label{tab:model9_comparison}
\end{table}

Model 9's advantages:
\begin{itemize}
    \item Comparable or superior predictive accuracy
    \item 100\% data utilization (no outlier removal)
    \item Automatic interaction detection
    \item Natural handling of non-linear relationships
    \item Built-in validation through OOB error
\end{itemize}

Trade-offs:
\begin{itemize}
    \item Longer training time (acceptable for annual recalibration)
    \item Requires explainability infrastructure for full interpretability
    \item Slightly more complex to explain to non-technical stakeholders
\end{itemize}

\subsection{Recommendations}

\subsubsection{Implementation Pathway}

\begin{enumerate}
    \item \textbf{Parallel Operation}: Run Model 9 alongside Model 5b for 6 months
    \item \textbf{Comparative Analysis}: Compare predictions for consistency and identify cases where models diverge
    \item \textbf{Explainability Development}: Implement SHAP values for individual prediction explanations
    \item \textbf{Stakeholder Communication}: Develop non-technical materials explaining Random Forest advantages
    \item \textbf{Gradual Transition}: Begin using Model 9 for new cases while maintaining Model 5b for existing consumers
    \item \textbf{Full Deployment}: After successful pilot, transition all predictions to Model 9
\end{enumerate}

\subsubsection{Ongoing Monitoring}

\begin{itemize}
    \item Track OOB error continuously to detect performance degradation
    \item Monitor feature importance changes to identify shifting cost drivers
    \item Review cases where predictions differ substantially from actual costs
    \item Retrain annually with updated data to maintain accuracy
\end{itemize}

\subsubsection{Future Enhancements}

\begin{itemize}
    \item \textbf{Hyperparameter Optimization}: Use cross-validation to tune number of trees, max depth, etc.
    \item \textbf{Uncertainty Quantification}: Use quantile regression forests to provide prediction intervals
    \item \textbf{Cost-Sensitive Learning}: Weight errors differently based on policy objectives
    \item \textbf{Online Learning}: Update model incrementally as new data becomes available
\end{itemize}

\section{Conclusion}

Model 9 demonstrates that Random Forest regression offers a compelling alternative to traditional linear models for iBudget prediction. Its automatic interaction detection, natural robustness to outliers, and strong predictive performance make it well-suited for the complex, heterogeneous nature of disability support costs. The 100\% data utilization ensures equitable treatment of all consumers, while feature importance rankings provide valuable policy insights. With appropriate explainability infrastructure, Model 9 represents a viable path forward for next-generation iBudget allocation.